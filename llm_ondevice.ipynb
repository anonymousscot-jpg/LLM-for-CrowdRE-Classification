{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb58b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 17:54:16,720 - INFO - Starting Requirements Classification Analysis\n",
      "2025-06-12 17:54:16,721 - INFO - Configuration: Config(csv_path='/Users/SONY/Documents/College/Study_pdeu/Summer_internship/IISER_B/Arpit sir/LLM_test/requirements_merged.csv', model_name='phi4-mini-reasoning:3.8b', valid_categories=['Energy', 'Entertainment', 'Health', 'Safety', 'Other'], sample_sizes=[10], num_repeats=1, delay_between_requests=0.1, max_retries=1, output_dir='classification_results')\n",
      "2025-06-12 17:54:16,742 - INFO - Loaded dataset with 2966 requirements\n",
      "2025-06-12 17:54:16,747 - INFO - Dataset ready with 2966 valid requirements.\n",
      "2025-06-12 17:54:16,747 - INFO - Starting comprehensive experiments: 5 total runs\n",
      "2025-06-12 17:54:16,747 - INFO - \n",
      "============================================================\n",
      "TECHNIQUE 1/5: Context Manager\n",
      "============================================================\n",
      "2025-06-12 17:54:16,747 - INFO -   Running: Sample Size=10, Repeat=1/1 (Exp 1/5)\n",
      "2025-06-12 17:54:39,815 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 17:55:24,948 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 17:55:42,230 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 17:55:58,808 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 17:56:20,834 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 17:56:45,605 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 17:57:15,841 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 17:57:59,434 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 17:58:37,867 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 17:59:08,749 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 17:59:08,858 - INFO -     -> Accuracy: 10.00% (1/10)\n",
      "2025-06-12 17:59:08,859 - INFO - \n",
      "============================================================\n",
      "TECHNIQUE 2/5: Domain Expert Persona\n",
      "============================================================\n",
      "2025-06-12 17:59:08,860 - INFO -   Running: Sample Size=10, Repeat=1/1 (Exp 2/5)\n",
      "2025-06-12 17:59:56,993 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 18:00:42,459 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 18:01:19,945 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 18:02:08,272 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 18:03:15,880 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 18:03:51,594 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 18:04:43,983 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 18:05:43,641 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 18:06:21,998 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 18:07:05,028 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 18:07:05,137 - INFO -     -> Accuracy: 30.00% (3/10)\n",
      "2025-06-12 18:07:05,140 - INFO - \n",
      "============================================================\n",
      "TECHNIQUE 3/5: Structured Chain of Thought\n",
      "============================================================\n",
      "2025-06-12 18:07:05,141 - INFO -   Running: Sample Size=10, Repeat=1/1 (Exp 3/5)\n",
      "2025-06-12 18:08:22,164 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 18:10:16,570 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 18:12:25,185 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 18:13:36,870 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 18:14:58,800 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 18:16:11,485 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 18:17:35,634 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 18:20:50,940 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 19:01:12,876 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 19:01:52,072 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 19:01:52,183 - INFO -     -> Accuracy: 40.00% (4/10)\n",
      "2025-06-12 19:01:52,184 - INFO - \n",
      "============================================================\n",
      "TECHNIQUE 4/5: Enhanced Zero-Shot\n",
      "============================================================\n",
      "2025-06-12 19:01:52,184 - INFO -   Running: Sample Size=10, Repeat=1/1 (Exp 4/5)\n",
      "2025-06-12 19:02:19,351 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 19:02:21,341 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 19:02:21,349 - WARNING - Could not extract category from response: PLAIN TEXT WITH HYPHENES\n",
      "\n",
      "Select one category for the requirement based on its primary purpose and d...\n",
      "2025-06-12 19:02:46,301 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 19:02:47,632 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 19:02:49,308 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 19:02:49,309 - WARNING - Could not extract category from response: PLAIN TEXT WITH HYPHENES\n",
      "\n",
      "SOFTWARE-REQUIREMENT-CATEGORY....\n",
      "2025-06-12 19:03:10,639 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 19:04:10,776 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 19:04:41,847 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 19:05:02,990 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 19:05:08,424 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 19:05:08,532 - INFO -     -> Accuracy: 30.00% (3/10)\n",
      "2025-06-12 19:05:08,535 - INFO - \n",
      "============================================================\n",
      "TECHNIQUE 5/5: Few-Shot with Reasoning\n",
      "============================================================\n",
      "2025-06-12 19:05:08,536 - INFO -   Running: Sample Size=10, Repeat=1/1 (Exp 5/5)\n",
      "2025-06-12 19:05:32,766 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 19:06:05,252 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 19:07:04,885 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 19:07:35,103 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 19:08:42,550 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 19:09:26,069 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 19:10:46,856 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 19:11:17,414 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 19:12:09,513 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 19:13:21,824 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-06-12 19:13:21,934 - INFO -     -> Accuracy: 30.00% (3/10)\n",
      "2025-06-12 19:13:21,938 - INFO - Generating comprehensive report...\n",
      "2025-06-12 19:13:22,374 - INFO - Raw data saved to CSV and JSON files.\n",
      "2025-06-12 19:13:22,375 - INFO - Report and data saved in 'classification_results/'\n",
      "2025-06-12 19:13:22,375 - INFO - Analysis completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "from ollama import Client\n",
    "import json\n",
    "import logging\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration class for the requirements classifier.\"\"\"\n",
    "    csv_path: str = '' # Replace with your CSV file path\n",
    "    model_name: str = \"phi4-mini-reasoning:3.8b\" # Change to your desired model\n",
    "    valid_categories: List[str] = None\n",
    "    sample_sizes: List[int] = None\n",
    "    num_repeats: int = 1\n",
    "    delay_between_requests: float = 0.1\n",
    "    max_retries: int = 1\n",
    "    output_dir: str = \"classification_results\"\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.valid_categories is None:\n",
    "            self.valid_categories = ['Energy', 'Entertainment', 'Health', 'Safety', 'Other']\n",
    "        if self.sample_sizes is None:\n",
    "            # Using a slightly larger sample size to make results more meaningful\n",
    "            self.sample_sizes = [10]\n",
    "\n",
    "# Initialize configuration\n",
    "config = Config()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# IMPROVED PROMPT PATTERNS\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class PromptTemplates:\n",
    "    \"\"\"Enhanced prompt templates with better structure and clarity.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_templates() -> List[Tuple[str, str]]:\n",
    "        \"\"\"Returns list of (name, template) tuples.\"\"\"\n",
    "        return [\n",
    "           (\"Context Manager\", \"\"\"\n",
    "You are an expert requirements analyst with deep knowledge of software domains.\n",
    "\n",
    "CLASSIFICATION TASK:\n",
    "Categorize this requirement into exactly one of:\n",
    "Energy | Entertainment | Health | Safety | Other\n",
    "\n",
    "CONTEXT GUIDELINES:\n",
    "• Consider primary business purpose and end-user impact.\n",
    "• Weigh explicit signals (keywords like “battery” or “fitness”) over incidental mentions.\n",
    "• If ambiguous, choose the category that best aligns with the main functional intent.\n",
    "\n",
    "REQUIREMENT: \"{req}\"\n",
    "\n",
    "STRUCTURED RESPONSE:\n",
    "1. Reasoning: [Briefly state (in 1–2 sentences) why you assigned the category.]\n",
    "2. Category: [One word: Energy, Entertainment, Health, Safety, or Other]\n",
    "\"\"\"),\n",
    "\n",
    "            (\"Domain Expert Persona\", \"\"\"\n",
    "As a senior requirements engineer with 15+ years in software specification, classify this requirement precisely.\n",
    "\n",
    "DOMAIN EXPERTISE:\n",
    "• Energy management (e.g., power optimization, battery usage)\n",
    "• Entertainment platforms (e.g., media streaming, game features)\n",
    "• Health software (e.g., patient monitoring, wellness tracking)\n",
    "• Safety systems (e.g., encryption, compliance, risk analysis)\n",
    "\n",
    "REQUIREMENT TO CLASSIFY: \"{req}\"\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "1. Expert Justification: [One clear sentence linking requirement to chosen domain.]\n",
    "2. Final Category: [One word: Energy | Entertainment | Health | Safety | Other]\n",
    "\"\"\"),\n",
    "\n",
    "            (\"Structured Chain of Thought\", \"\"\"\n",
    "REQUIREMENT CLASSIFICATION SYSTEM\n",
    "\n",
    "CATEGORIES (choose exactly one):\n",
    "• Energy – Power/battery management or energy efficiency\n",
    "• Entertainment – Media streaming, gaming, or user engagement features\n",
    "• Health – Clinical, medical device, fitness, or wellness applications\n",
    "• Safety – Data protection, compliance, or risk mitigation\n",
    "• Other – Requirements that do not clearly fit above\n",
    "\n",
    "CLASSIFICATION PROCESS:\n",
    "1. Extract Key Terms: [Identify three to five primary verbs/nouns]\n",
    "2. Primary Stakeholder: [Who benefits most? e.g., end user, clinician, operator]\n",
    "3. Core Function: [What capability does this requirement enable?]\n",
    "4. Business Value: [Why is this requirement needed? e.g., cost reduction, increased safety]\n",
    "\n",
    "REQUIREMENT: \"{req}\"\n",
    "\n",
    "ANALYSIS:\n",
    "• Key Terms:\n",
    "• Primary Stakeholder:\n",
    "• Core Function:\n",
    "• Business Value:\n",
    "\n",
    "FINAL CATEGORY: [Energy | Entertainment | Health | Safety | Other]\n",
    "\"\"\"),\n",
    "\n",
    "            (\"Enhanced Zero-Shot\", \"\"\"\n",
    "SOFTWARE REQUIREMENT CLASSIFICATION\n",
    "\n",
    "Choose exactly one category based on the requirement’s primary purpose and domain:\n",
    "\n",
    "• Energy – References to power, battery, consumption, or energy management\n",
    "• Entertainment – Mentions of games, media, streaming, social, or recreational features\n",
    "• Health – Indicators of medical/clinical, fitness, wellness, or patient care\n",
    "• Safety – Concerns about security, protection, compliance, or risk mitigation\n",
    "• Other – Requirements not primarily about the above domains\n",
    "\n",
    "REQUIREMENT: \"{req}\"\n",
    "\n",
    "CLASSIFICATION (one word only): [Energy | Entertainment | Health | Safety | Other]\n",
    "\"\"\"),\n",
    "\n",
    "            (\"Few-Shot with Reasoning\", \"\"\"\n",
    "CLASSIFICATION EXAMPLES WITH EXPLANATION:\n",
    "\n",
    "Example 1:\n",
    "Requirement: \"The smart thermostat shall learn user preferences and optimize heating schedules to minimize energy consumption while maintaining comfort.\"\n",
    "Reasoning: The emphasis is clearly on reducing energy usage via automated scheduling.\n",
    "Category: Energy\n",
    "\n",
    "Example 2:\n",
    "Requirement: \"The fitness app shall provide real-time heart rate monitoring and alert users when heart rate exceeds safe thresholds during exercise.\"\n",
    "Reasoning: Core functionality relates to health monitoring and user safety during workouts.\n",
    "Category: Health\n",
    "\n",
    "Example 3:\n",
    "Requirement: \"The streaming service shall recommend personalized content based on viewing history and allow users to create custom playlists.\"\n",
    "Reasoning: Focuses on user engagement by delivering entertainment-oriented features.\n",
    "Category: Entertainment\n",
    "\n",
    "TASK: Classify the following requirement following the same format:\n",
    "\n",
    "Requirement: \"{req}\"\n",
    "\n",
    "Reasoning: [One sentence explaining why]\n",
    "Category: [Energy | Entertainment | Health | Safety | Other]\n",
    "\"\"\")\n",
    "        ]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# ENHANCED CLASSIFICATION SYSTEM\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class RequirementsClassifier:\n",
    "    \"\"\"Enhanced requirements classification system with improved error handling and response parsing.\"\"\"\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.client = Client()\n",
    "        self.prompt_templates = PromptTemplates.get_templates()\n",
    "\n",
    "    def _extract_category_from_response(self, response: str) -> str:\n",
    "        \"\"\"Enhanced category extraction with multiple fallback strategies.\"\"\"\n",
    "        response = response.strip()\n",
    "\n",
    "        # Strategy 1: Look for exact category matches (case-insensitive)\n",
    "        for category in self.config.valid_categories:\n",
    "            pattern = rf'\\b{re.escape(category)}\\b'\n",
    "            if re.search(pattern, response, re.IGNORECASE):\n",
    "                return category\n",
    "\n",
    "        # Strategy 2: Look for category after common prefixes\n",
    "        prefixes = ['category:', 'classification:', 'answer:', 'final:', 'result:', 'consensus category:']\n",
    "        for prefix in prefixes:\n",
    "            pattern = rf'{prefix}\\s*(\\w+)'\n",
    "            match = re.search(pattern, response, re.IGNORECASE)\n",
    "            if match:\n",
    "                candidate = match.group(1).capitalize().strip()\n",
    "                if candidate in self.config.valid_categories:\n",
    "                    return candidate\n",
    "\n",
    "        # Strategy 3: Check last line for category\n",
    "        lines = response.split('\\n')\n",
    "        for line in reversed(lines):\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                for category in self.config.valid_categories:\n",
    "                    if category.lower() in line.lower():\n",
    "                        return category\n",
    "\n",
    "        # Strategy 4: Return first token if it matches a category\n",
    "        first_token = response.split()[0] if response.split() else \"\"\n",
    "        first_token_cap = first_token.capitalize()\n",
    "        if first_token_cap in self.config.valid_categories:\n",
    "            return first_token_cap\n",
    "\n",
    "        logger.warning(f\"Could not extract category from response: {response[:100]}...\")\n",
    "        return \"Other\"\n",
    "\n",
    "    def classify_requirement(self, prompt_template: str, requirement_text: str) -> Dict:\n",
    "        \"\"\"Classify a single requirement with enhanced error handling.\"\"\"\n",
    "        for attempt in range(self.config.max_retries):\n",
    "            try:\n",
    "                prompt = prompt_template.format(req=requirement_text.strip())\n",
    "\n",
    "                response = self.client.generate(\n",
    "                    model=self.config.model_name,\n",
    "                    prompt=prompt,\n",
    "                    options={\n",
    "                        'temperature': 1.0,\n",
    "                        'top_p': 0.9,\n",
    "                        'num_ctx': 16384\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                raw_response = response.get('response', '').strip()\n",
    "                predicted_category = self._extract_category_from_response(raw_response)\n",
    "\n",
    "                return {\n",
    "                    'predicted_category': predicted_category,\n",
    "                    'raw_response': raw_response,\n",
    "                    'success': True,\n",
    "                    'attempt': attempt + 1\n",
    "                }\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Attempt {attempt + 1} failed for model '{self.config.model_name}': {str(e)}\")\n",
    "                if attempt < self.config.max_retries - 1:\n",
    "                    time.sleep(1)\n",
    "                else:\n",
    "                    return {\n",
    "                        'predicted_category': 'Other',\n",
    "                        'raw_response': f\"Error: {str(e)}\",\n",
    "                        'success': False,\n",
    "                        'attempt': attempt + 1\n",
    "                    }\n",
    "\n",
    "        return {\n",
    "            'predicted_category': 'Other',\n",
    "            'raw_response': \"Max retries exceeded\",\n",
    "            'success': False,\n",
    "            'attempt': self.config.max_retries\n",
    "        }\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# COMPREHENSIVE EXPERIMENT RUNNER\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class ComprehensiveExperimentRunner:\n",
    "    \"\"\"Runs comprehensive experiments across all techniques and sample sizes.\"\"\"\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.classifier = RequirementsClassifier(config)\n",
    "        self.df = None\n",
    "        self.results = []\n",
    "\n",
    "        Path(config.output_dir).mkdir(exist_ok=True)\n",
    "\n",
    "    def load_dataset(self) -> None:\n",
    "        \"\"\"Load and validate the dataset.\"\"\"\n",
    "        try:\n",
    "            self.df = pd.read_csv(self.config.csv_path)\n",
    "            required_columns = ['requirements', 'application_domain']\n",
    "\n",
    "            for col in required_columns:\n",
    "                if col not in self.df.columns:\n",
    "                    raise ValueError(f\"CSV must contain '{col}' column\")\n",
    "\n",
    "            logger.info(f\"Loaded dataset with {len(self.df)} requirements\")\n",
    "\n",
    "            self.df.dropna(subset=required_columns, inplace=True)\n",
    "            self.df['application_domain'] = self.df['application_domain'].str.strip().str.capitalize()\n",
    "\n",
    "            valid_mask = self.df['application_domain'].isin(self.config.valid_categories)\n",
    "            if not valid_mask.all():\n",
    "                invalid_categories = self.df.loc[~valid_mask, 'application_domain'].unique()\n",
    "                logger.warning(f\"Found and filtered out invalid categories: {invalid_categories}\")\n",
    "                self.df = self.df[valid_mask]\n",
    "\n",
    "            logger.info(f\"Dataset ready with {len(self.df)} valid requirements.\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"Dataset file not found at: {self.config.csv_path}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load dataset: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def run_comprehensive_experiments(self) -> None:\n",
    "        \"\"\"Run experiments for all techniques across all sample sizes.\"\"\"\n",
    "        if self.df is None:\n",
    "            self.load_dataset()\n",
    "\n",
    "        total_experiments = len(self.classifier.prompt_templates) * len(self.config.sample_sizes) * self.config.num_repeats\n",
    "        current_experiment = 0\n",
    "\n",
    "        logger.info(f\"Starting comprehensive experiments: {total_experiments} total runs\")\n",
    "\n",
    "        for technique_idx, (technique_name, prompt_template) in enumerate(self.classifier.prompt_templates, 1):\n",
    "            logger.info(f\"\\n{'='*60}\\nTECHNIQUE {technique_idx}/{len(self.classifier.prompt_templates)}: {technique_name}\\n{'='*60}\")\n",
    "\n",
    "            for sample_size in self.config.sample_sizes:\n",
    "                for repeat_idx in range(1, self.config.num_repeats + 1):\n",
    "                    current_experiment += 1\n",
    "                    logger.info(f\"  Running: Sample Size={sample_size}, Repeat={repeat_idx}/{self.config.num_repeats} (Exp {current_experiment}/{total_experiments})\")\n",
    "\n",
    "                    actual_sample_size = min(sample_size, len(self.df))\n",
    "                    sample_df = self.df.sample(n=actual_sample_size, random_state=random.randint(1, 1_000_000))\n",
    "\n",
    "                    experiment_result = self._run_single_experiment(\n",
    "                        technique_idx, technique_name, prompt_template,\n",
    "                        sample_size, repeat_idx, sample_df\n",
    "                    )\n",
    "\n",
    "                    self.results.append(experiment_result)\n",
    "                    accuracy = experiment_result['accuracy']\n",
    "                    logger.info(f\"    -> Accuracy: {accuracy:.2%} ({experiment_result['num_correct']}/{experiment_result['num_samples']})\")\n",
    "\n",
    "    def _run_single_experiment(self, technique_idx: int, technique_name: str,\n",
    "                             prompt_template: str, sample_size: int, repeat_idx: int,\n",
    "                             sample_df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Run a single experiment configuration.\"\"\"\n",
    "        correct_count = 0\n",
    "        failed_count = 0\n",
    "        per_req_outcomes = []\n",
    "        category_confusion = {cat: {cat2: 0 for cat2 in self.config.valid_categories}\n",
    "                            for cat in self.config.valid_categories}\n",
    "\n",
    "        for idx, row in sample_df.iterrows():\n",
    "            req_text = row['requirements']\n",
    "            true_label = row['application_domain']\n",
    "\n",
    "            result = self.classifier.classify_requirement(prompt_template, req_text)\n",
    "            predicted_label = result['predicted_category']\n",
    "\n",
    "            is_correct = (predicted_label == true_label)\n",
    "            if is_correct:\n",
    "                correct_count += 1\n",
    "            if not result['success']:\n",
    "                failed_count += 1\n",
    "\n",
    "            if true_label in category_confusion and predicted_label in category_confusion[true_label]:\n",
    "                category_confusion[true_label][predicted_label] += 1\n",
    "\n",
    "            per_req_outcomes.append({\n",
    "                'requirement_id': idx,\n",
    "                'requirement': req_text,\n",
    "                'true_label': true_label,\n",
    "                'predicted_label': predicted_label,\n",
    "                'correct': is_correct,\n",
    "                'success': result['success']\n",
    "            })\n",
    "\n",
    "            time.sleep(self.config.delay_between_requests)\n",
    "\n",
    "        total = len(sample_df)\n",
    "        accuracy = correct_count / total if total > 0 else 0\n",
    "\n",
    "        return {\n",
    "            'technique_index': technique_idx, 'technique_name': technique_name,\n",
    "            'sample_size': sample_size, 'repeat_index': repeat_idx,\n",
    "            'num_samples': total, 'num_correct': correct_count, 'num_failed': failed_count,\n",
    "            'accuracy': accuracy, 'failure_rate': failed_count / total if total > 0 else 0,\n",
    "            'confusion_matrix': category_confusion, 'per_requirement_results': per_req_outcomes\n",
    "        }\n",
    "\n",
    "    def generate_comprehensive_report(self) -> None:\n",
    "        \"\"\"Generate comprehensive analysis report with visualizations.\"\"\"\n",
    "        logger.info(\"Generating comprehensive report...\")\n",
    "\n",
    "        if not self.results:\n",
    "            logger.warning(\"No results to report. Aborting report generation.\")\n",
    "            return\n",
    "\n",
    "        summary_df = self._create_summary_dataframe()\n",
    "        detailed_df = self._create_detailed_dataframe()\n",
    "\n",
    "        technique_metrics = self._analyze_technique_metrics()\n",
    "\n",
    "        report_sections = {\n",
    "            'executive_summary': self._generate_executive_summary(summary_df),\n",
    "            'technique_comparison': self._generate_technique_comparison(summary_df, technique_metrics),\n",
    "            'sample_size_analysis': self._generate_sample_size_analysis(summary_df),\n",
    "            'confusion_analysis': self._generate_confusion_analysis(),\n",
    "            'recommendations': self._generate_recommendations(summary_df)\n",
    "        }\n",
    "\n",
    "        self._create_visualizations(summary_df)\n",
    "        self._generate_html_report(report_sections, summary_df)\n",
    "        self._save_raw_data(summary_df, detailed_df)\n",
    "\n",
    "        logger.info(f\"Report and data saved in '{self.config.output_dir}/'\")\n",
    "\n",
    "    # --- Dataframe and Metric Calculation ---\n",
    "\n",
    "    def _create_summary_dataframe(self) -> pd.DataFrame:\n",
    "        \"\"\"Create summary dataframe for analysis.\"\"\"\n",
    "        return pd.DataFrame([{\n",
    "            'technique_index': r['technique_index'], 'technique_name': r['technique_name'],\n",
    "            'sample_size': r['sample_size'], 'repeat_index': r['repeat_index'],\n",
    "            'accuracy': r['accuracy'], 'failure_rate': r['failure_rate'],\n",
    "            'num_samples': r['num_samples'], 'num_correct': r['num_correct']\n",
    "        } for r in self.results])\n",
    "\n",
    "    def _create_detailed_dataframe(self) -> pd.DataFrame:\n",
    "        \"\"\"Create detailed dataframe with per-requirement results.\"\"\"\n",
    "        detailed_data = []\n",
    "        for r in self.results:\n",
    "            for req in r['per_requirement_results']:\n",
    "                detailed_data.append({\n",
    "                    'technique_name': r['technique_name'], 'sample_size': r['sample_size'],\n",
    "                    'repeat_index': r['repeat_index'], 'requirement_id': req['requirement_id'],\n",
    "                    'true_label': req['true_label'], 'predicted_label': req['predicted_label'],\n",
    "                    'correct': req['correct'], 'success': req['success']\n",
    "                })\n",
    "        return pd.DataFrame(detailed_data)\n",
    "\n",
    "    def _calculate_metrics_from_confusion_matrix(self, cm: Dict[str, Dict[str, int]]) -> Dict[str, float]:\n",
    "        \"\"\"Calculates precision, recall, and F-scores from a confusion matrix.\"\"\"\n",
    "        metrics = {}\n",
    "        total_tp, total_fp, total_fn, total_samples = 0, 0, 0, 0\n",
    "\n",
    "        for cat in self.config.valid_categories:\n",
    "            tp = cm.get(cat, {}).get(cat, 0)\n",
    "            fp = sum(cm.get(other_cat, {}).get(cat, 0) for other_cat in self.config.valid_categories if other_cat != cat)\n",
    "            fn = sum(cm.get(cat, {}).get(other_cat, 0) for other_cat in self.config.valid_categories if other_cat != cat)\n",
    "            support = tp + fn\n",
    "\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if support > 0 else 0\n",
    "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            f2 = 5 * precision * recall / (4 * precision + recall) if (4 * precision + recall) > 0 else 0\n",
    "\n",
    "            metrics[cat] = {'precision': precision, 'recall': recall, 'f1': f1, 'f2': f2, 'support': support}\n",
    "            total_tp += tp\n",
    "            total_fp += fp\n",
    "            total_fn += fn\n",
    "            total_samples += support\n",
    "\n",
    "        # Overall metrics\n",
    "        macro_f1 = sum(m['f1'] for m in metrics.values()) / len(self.config.valid_categories)\n",
    "        macro_precision = sum(m['precision'] for m in metrics.values()) / len(self.config.valid_categories)\n",
    "        macro_recall = sum(m['recall'] for m in metrics.values()) / len(self.config.valid_categories)\n",
    "\n",
    "        weighted_f1 = sum(m['f1'] * m['support'] for m in metrics.values()) / total_samples if total_samples > 0 else 0\n",
    "\n",
    "        return {\n",
    "            'per_category': metrics,\n",
    "            'macro_f1': macro_f1, 'macro_precision': macro_precision, 'macro_recall': macro_recall,\n",
    "            'weighted_f1': weighted_f1, 'accuracy': total_tp / total_samples if total_samples > 0 else 0\n",
    "        }\n",
    "\n",
    "    def _analyze_technique_metrics(self) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"Aggregates confusion matrices and calculates metrics for each technique.\"\"\"\n",
    "        technique_cms = {}\n",
    "        for result in self.results:\n",
    "            tech_name = result['technique_name']\n",
    "            if tech_name not in technique_cms:\n",
    "                technique_cms[tech_name] = {cat: {c2: 0 for c2 in self.config.valid_categories} for cat in self.config.valid_categories}\n",
    "\n",
    "            cm = result['confusion_matrix']\n",
    "            for true_cat, preds in cm.items():\n",
    "                for pred_cat, count in preds.items():\n",
    "                    if true_cat in technique_cms[tech_name] and pred_cat in technique_cms[tech_name][true_cat]:\n",
    "                        technique_cms[tech_name][true_cat][pred_cat] += count\n",
    "\n",
    "        technique_metrics = {}\n",
    "        for tech_name, agg_cm in technique_cms.items():\n",
    "            metrics = self._calculate_metrics_from_confusion_matrix(agg_cm)\n",
    "            technique_metrics[tech_name] = {k: v for k, v in metrics.items() if k != 'per_category'}\n",
    "\n",
    "        return technique_metrics\n",
    "\n",
    "    # --- Report Generation Sections ---\n",
    "\n",
    "    def _generate_executive_summary(self, summary_df: pd.DataFrame) -> str:\n",
    "        \"\"\"Generate executive summary section.\"\"\"\n",
    "        avg_accuracy = summary_df['accuracy'].mean()\n",
    "        technique_perf = summary_df.groupby('technique_name')['accuracy'].mean()\n",
    "        best_technique = technique_perf.idxmax()\n",
    "        best_accuracy = technique_perf.max()\n",
    "\n",
    "        return f\"\"\"\n",
    "        <h2>Executive Summary</h2>\n",
    "        <p>This report analyzes the performance of {summary_df['technique_name'].nunique()} prompt engineering techniques for software requirements classification.</p>\n",
    "        <h3>Key Findings</h3>\n",
    "        <ul>\n",
    "            <li><b>Overall Average Accuracy:</b> {avg_accuracy:.2%} across all experiments.</li>\n",
    "            <li><b>Best Performing Technique:</b> <b>{best_technique}</b>, with an average accuracy of <b>{best_accuracy:.2%}</b>.</li>\n",
    "            <li><b>Experiment Scope:</b> {len(summary_df)} individual runs were conducted across {len(self.config.sample_sizes)} sample size(s) ({self.config.sample_sizes}) with {self.config.num_repeats} repeat(s) each.</li>\n",
    "        </ul>\n",
    "        \"\"\"\n",
    "\n",
    "    def _generate_technique_comparison(self, summary_df: pd.DataFrame, technique_metrics: Dict) -> str:\n",
    "        \"\"\"Generate technique comparison analysis, now with F1 scores.\"\"\"\n",
    "        technique_stats = summary_df.groupby('technique_name').agg(\n",
    "            accuracy_mean=('accuracy', 'mean'),\n",
    "            accuracy_std=('accuracy', 'std'),\n",
    "        ).reset_index()\n",
    "\n",
    "        technique_stats['accuracy_std'] = technique_stats['accuracy_std'].fillna(0)\n",
    "\n",
    "        metrics_df = pd.DataFrame.from_dict(technique_metrics, orient='index').reset_index().rename(columns={'index': 'technique_name'})\n",
    "        technique_stats = pd.merge(technique_stats, metrics_df, on='technique_name')\n",
    "\n",
    "        technique_stats = technique_stats.sort_values('accuracy_mean', ascending=False)\n",
    "\n",
    "        comparison = \"\"\"\n",
    "        <h2>Technique Comparison Analysis</h2>\n",
    "        <h3>Performance Ranking</h3>\n",
    "        <p>Techniques are ranked by their average accuracy. Macro-averaged metrics provide a balanced view of performance across all categories.</p>\n",
    "        <table>\n",
    "            <thead>\n",
    "                <tr>\n",
    "                    <th>Rank</th><th>Technique</th><th>Avg Accuracy</th><th>Std Dev</th><th>Macro F1-Score</th><th>Macro Precision</th><th>Macro Recall</th>\n",
    "                </tr>\n",
    "            </thead>\n",
    "            <tbody>\n",
    "        \"\"\"\n",
    "\n",
    "        for rank, row in enumerate(technique_stats.itertuples(), 1):\n",
    "            comparison += (f\"<tr><td>{rank}</td><td>{row.technique_name}</td><td>{row.accuracy_mean:.2%}</td><td>{row.accuracy_std:.3f}</td>\"\n",
    "                           f\"<td><b>{row.macro_f1:.2%}</b></td><td>{row.macro_precision:.2%}</td><td>{row.macro_recall:.2%}</td></tr>\\n\")\n",
    "\n",
    "        comparison += \"</tbody></table>\"\n",
    "\n",
    "        best_technique = technique_stats.iloc[0]\n",
    "        if self.config.num_repeats > 1:\n",
    "            most_consistent = technique_stats.sort_values('accuracy_std').iloc[0]\n",
    "            consistency_text = f\"<b>Most Consistent:</b> {most_consistent.technique_name} (Std Dev: {most_consistent.accuracy_std:.3f})\"\n",
    "        else:\n",
    "            consistency_text = \"<b>Consistency:</b> Not applicable (requires >1 repeat to measure).\"\n",
    "\n",
    "        comparison += f\"\"\"\n",
    "        <h3>Key Observations</h3>\n",
    "        <ul>\n",
    "            <li><b>Top Performer:</b> {best_technique.technique_name} leads in both accuracy and F1-score.</li>\n",
    "            <li>{consistency_text}</li>\n",
    "        </ul>\n",
    "        \"\"\"\n",
    "        return comparison\n",
    "\n",
    "    def _generate_sample_size_analysis(self, summary_df: pd.DataFrame) -> str:\n",
    "        \"\"\"Generate sample size impact analysis.\"\"\"\n",
    "        if len(self.config.sample_sizes) < 2:\n",
    "            return \"<h2>Sample Size Impact Analysis</h2><p>Not applicable (only one sample size was tested).</p>\"\n",
    "\n",
    "        size_stats = summary_df.groupby('sample_size')['accuracy'].agg(['mean', 'std']).reset_index()\n",
    "        analysis = \"<h2>Sample Size Impact Analysis</h2><table><thead><tr><th>Sample Size</th><th>Avg Accuracy</th><th>Std Dev</th></tr></thead><tbody>\"\n",
    "        for row in size_stats.itertuples():\n",
    "            analysis += f\"<tr><td>{row.sample_size}</td><td>{row.mean:.2%}</td><td>{row.std:.3f}</td></tr>\"\n",
    "        analysis += \"</tbody></table>\"\n",
    "        return analysis\n",
    "\n",
    "    def _generate_confusion_analysis(self) -> str:\n",
    "        \"\"\"Generate confusion matrix analysis with overall metrics.\"\"\"\n",
    "        total_cm = {cat: {c2: 0 for c2 in self.config.valid_categories} for cat in self.config.valid_categories}\n",
    "        for result in self.results:\n",
    "            for true_cat, preds in result['confusion_matrix'].items():\n",
    "                for pred_cat, count in preds.items():\n",
    "                     if true_cat in total_cm and pred_cat in total_cm[true_cat]:\n",
    "                        total_cm[true_cat][pred_cat] += count\n",
    "\n",
    "        metrics = self._calculate_metrics_from_confusion_matrix(total_cm)\n",
    "\n",
    "        analysis = \"<h2>Overall Classification Performance</h2><h3>Confusion Matrix</h3><table><thead><tr><th>True \\\\ Pred</th>\"\n",
    "        for cat in self.config.valid_categories:\n",
    "            analysis += f\"<th>{cat}</th>\"\n",
    "        analysis += \"</tr></thead><tbody>\"\n",
    "\n",
    "        for true_cat in self.config.valid_categories:\n",
    "            analysis += f\"<tr><td><b>{true_cat}</b></td>\"\n",
    "            for pred_cat in self.config.valid_categories:\n",
    "                analysis += f\"<td>{total_cm[true_cat][pred_cat]}</td>\"\n",
    "            analysis += \"</tr>\"\n",
    "        analysis += \"</tbody></table>\"\n",
    "\n",
    "        analysis += \"<h3>Per-Category Performance</h3><table><thead><tr><th>Category</th><th>Precision</th><th>Recall</th><th>F1-Score</th><th>F2-Score</th><th>Support</th></tr></thead><tbody>\"\n",
    "        for cat, m in metrics['per_category'].items():\n",
    "            analysis += f\"<tr><td>{cat}</td><td>{m['precision']:.2%}</td><td>{m['recall']:.2%}</td><td>{m['f1']:.2%}</td><td>{m['f2']:.2%}</td><td>{m['support']}</td></tr>\"\n",
    "\n",
    "        analysis += f\"\"\"\n",
    "            </tbody><tfoot>\n",
    "                <tr style=\"font-weight:bold;\"><td>Macro Avg</td><td>{metrics['macro_precision']:.2%}</td><td>{metrics['macro_recall']:.2%}</td><td>{metrics['macro_f1']:.2%}</td><td>-</td><td>{sum(m['support'] for m in metrics['per_category'].values())}</td></tr>\n",
    "                <tr style=\"font-weight:bold;\"><td>Weighted Avg</td><td>-</td><td>-</td><td>{metrics['weighted_f1']:.2%}</td><td>-</td><td>-</td></tr>\n",
    "            </tfoot></table>\n",
    "        \"\"\"\n",
    "        return analysis\n",
    "\n",
    "    def _generate_recommendations(self, summary_df: pd.DataFrame) -> str:\n",
    "        \"\"\"Generate recommendations based on analysis.\"\"\"\n",
    "        technique_perf = summary_df.groupby('technique_name')['accuracy'].mean()\n",
    "        best_technique = technique_perf.idxmax()\n",
    "\n",
    "        return f\"\"\"\n",
    "        <h2>Recommendations</h2>\n",
    "        <ol>\n",
    "            <li><b>Optimal Technique:</b> Based on this analysis, the <b>{best_technique}</b> prompt is recommended for its superior performance in both accuracy and F1-score.</li>\n",
    "            <li><b>Further Testing:</b> To improve confidence, consider re-running experiments with a larger number of repeats (e.g., `num_repeats = 5`) and more varied sample sizes.</li>\n",
    "            <li><b>Error Analysis:</b> A detailed review of misclassifications (available in `detailed_results.csv`) for the top-performing techniques can provide insights for further prompt refinement.</li>\n",
    "        </ol>\n",
    "        \"\"\"\n",
    "\n",
    "    # --- Visualization and File Saving ---\n",
    "\n",
    "    def _create_visualizations(self, summary_df: pd.DataFrame) -> None:\n",
    "        \"\"\"Create and save visualization plots.\"\"\"\n",
    "        plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "        # Plot 1: Technique Comparison (Accuracy)\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        tech_perf = summary_df.groupby('technique_name')['accuracy'].mean().sort_values()\n",
    "        ax.barh(tech_perf.index, tech_perf.values, color='skyblue')\n",
    "        ax.set_xlabel('Mean Accuracy')\n",
    "        ax.set_title('Technique Performance by Mean Accuracy')\n",
    "        ax.set_xlim(0, 1)\n",
    "        for i, v in enumerate(tech_perf.values):\n",
    "            ax.text(v + 0.01, i, f\"{v:.2%}\", va='center', fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{self.config.output_dir}/technique_accuracy.png', dpi=150)\n",
    "        plt.close(fig)\n",
    "\n",
    "        # Plot 2: Sample Size Impact\n",
    "        if len(self.config.sample_sizes) > 1:\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            sns.lineplot(data=summary_df, x='sample_size', y='accuracy', marker='o', ax=ax, errorbar='sd')\n",
    "            ax.set_title('Impact of Sample Size on Accuracy')\n",
    "            ax.set_xlabel('Sample Size')\n",
    "            ax.set_ylabel('Mean Accuracy (with Std Dev)')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'{self.config.output_dir}/sample_size_impact.png', dpi=150)\n",
    "            plt.close(fig)\n",
    "\n",
    "    def _generate_html_report(self, report_sections: Dict[str, str], summary_df: pd.DataFrame) -> None:\n",
    "        \"\"\"Generate comprehensive HTML report.\"\"\"\n",
    "        html_content = f\"\"\"\n",
    "        <!DOCTYPE html><html lang=\"en\"><head><meta charset=\"UTF-8\"><title>Requirements Classification Analysis</title>\n",
    "        <style>\n",
    "            body {{ font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, Helvetica, Arial, sans-serif; margin: 0; background: #f8f9fa; }}\n",
    "            .container {{ max-width: 1200px; margin: 2em auto; background: #fff; padding: 2em; box-shadow: 0 0 20px rgba(0,0,0,0.05); border-radius: 8px; }}\n",
    "            h1, h2, h3 {{ color: #343a40; border-bottom: 2px solid #dee2e6; padding-bottom: 0.3em; }}\n",
    "            h1 {{ font-size: 2.5em; }} h2 {{ font-size: 1.8em; margin-top: 1.5em;}}\n",
    "            table {{ width: 100%; border-collapse: collapse; margin-top: 1em; }}\n",
    "            th, td {{ padding: 0.8em; text-align: left; border: 1px solid #dee2e6; }}\n",
    "            th {{ background-color: #f8f9fa; font-weight: 600; }}\n",
    "            tbody tr:nth-child(odd) {{ background-color: #f8f9fa; }}\n",
    "            img {{ max-width: 100%; height: auto; display: block; margin: 2em 0; border: 1px solid #dee2e6; border-radius: 4px; }}\n",
    "            .timestamp {{ color: #6c757d; font-size: 0.9em; }}\n",
    "        </style></head><body><div class=\"container\">\n",
    "            <h1>Requirements Classification Analysis Report</h1>\n",
    "            <p class=\"timestamp\">Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\n",
    "            {report_sections.get('executive_summary', '')}\n",
    "            <h2>Visual Analysis</h2>\n",
    "            <img src=\"technique_accuracy.png\" alt=\"Technique Accuracy Comparison\">\n",
    "            { '<img src=\"sample_size_impact.png\" alt=\"Sample Size Impact\">' if len(self.config.sample_sizes) > 1 else '' }\n",
    "            {report_sections.get('technique_comparison', '')}\n",
    "            {report_sections.get('sample_size_analysis', '')}\n",
    "            {report_sections.get('confusion_analysis', '')}\n",
    "            {report_sections.get('recommendations', '')}\n",
    "        </div></body></html>\n",
    "        \"\"\"\n",
    "        with open(f'{self.config.output_dir}/comprehensive_report.html', 'w', encoding='utf-8') as f:\n",
    "            f.write(html_content)\n",
    "\n",
    "    def _save_raw_data(self, summary_df: pd.DataFrame, detailed_df: pd.DataFrame) -> None:\n",
    "        \"\"\"Save raw experimental data to CSV and JSON files.\"\"\"\n",
    "        summary_df.to_csv(f'{self.config.output_dir}/summary_results.csv', index=False)\n",
    "        detailed_df.to_csv(f'{self.config.output_dir}/detailed_results.csv', index=False)\n",
    "\n",
    "        config_dict = {k: v for k, v in self.config.__dict__.items()}\n",
    "        experiment_log = {'config': config_dict, 'results': self.results}\n",
    "\n",
    "        with open(f'{self.config.output_dir}/experiment_log.json', 'w') as f:\n",
    "            json.dump(experiment_log, f, indent=2, default=str)\n",
    "\n",
    "        logger.info(\"Raw data saved to CSV and JSON files.\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# MAIN EXECUTION FUNCTION\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    try:\n",
    "        logger.info(\"Starting Requirements Classification Analysis\")\n",
    "        logger.info(f\"Configuration: {config}\")\n",
    "\n",
    "        runner = ComprehensiveExperimentRunner(config)\n",
    "        runner.run_comprehensive_experiments()\n",
    "        runner.generate_comprehensive_report()\n",
    "\n",
    "        logger.info(\"Analysis completed successfully!\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"CRITICAL ERROR: The dataset file was not found at the specified path in the configuration. Please check `config.csv_path`.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An unexpected error occurred during the analysis: {str(e)}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "     main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
